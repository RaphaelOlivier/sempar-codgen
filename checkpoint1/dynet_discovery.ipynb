{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = dy.ParameterCollection()\n",
    "pW = m.add_parameters((8,2))\n",
    "pV = m.add_parameters((1,8))\n",
    "pb = m.add_parameters((8))\n",
    "\n",
    "dy.renew_cg() # new computation graph. not strictly needed here, but good practice.\n",
    "\n",
    "# associate the parameters with cg Expressions\n",
    "W = dy.parameter(pW)\n",
    "V = dy.parameter(pV)\n",
    "b = dy.parameter(pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39957764744758606,\n",
       " -0.13521379232406616,\n",
       " -0.3813660740852356,\n",
       " -0.37368789315223694,\n",
       " 0.5874922871589661,\n",
       " 0.02694915048778057,\n",
       " 0.266682893037796,\n",
       " 0.5391783118247986]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dy.vecInput(2) # an input vector of size 2. Also an expression.\n",
    "output = dy.logistic(V*(dy.tanh((W*x)+b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47301405668258667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.set([0,0])\n",
    "output.value()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6405814290046692"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dy.scalarInput(0) # this will hold the correct answer\n",
    "loss = dy.binary_log_loss(output, y)\n",
    "loss.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7440601587295532\n",
      "0.6447012424468994\n",
      "0.6405814290046692\n"
     ]
    }
   ],
   "source": [
    "x.set([1,0])\n",
    "y.set(0)\n",
    "print (loss.value())\n",
    "\n",
    "y.set(1)\n",
    "print (loss.value())\n",
    "\n",
    "x.set([0,0])\n",
    "y.set(0)\n",
    "print (loss.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dy.SimpleSGDTrainer(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss before step is: 0.6447012424468994\n",
      "the loss after step is: 0.5604482889175415\n"
     ]
    }
   ],
   "source": [
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value() # this performs a forward through the network.\n",
    "print (\"the loss before step is:\",loss_value)\n",
    "\n",
    "# now do an optimization step\n",
    "loss.backward()  # compute the gradients\n",
    "trainer.update()\n",
    "\n",
    "# see how it affected the loss:\n",
    "loss_value = loss.value(recalculate=True) # recalculate=True means \"don't use precomputed value\"\n",
    "print (\"the loss after step is:\",loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = create_xor_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.43171693159639835\n",
      "average loss is: 0.24627967279590665\n",
      "average loss is: 0.1721019630841911\n",
      "average loss is: 0.13259905643644743\n",
      "average loss is: 0.10804088731147349\n",
      "average loss is: 0.09127307015533248\n",
      "average loss is: 0.07908279789738092\n",
      "average loss is: 0.069812831700081\n",
      "average loss is: 0.06252118222420622\n",
      "average loss is: 0.056632498042588124\n",
      "average loss is: 0.05177531759843061\n",
      "average loss is: 0.047698996481854314\n",
      "average loss is: 0.04422821692488371\n",
      "average loss is: 0.0412366271018483\n",
      "average loss is: 0.03863083712146617\n",
      "average loss is: 0.03634031140322622\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "seen_instances = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    x.set(question)\n",
    "    y.set(answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 500 == 0):\n",
    "        print (\"average loss is:\",total_loss / seen_instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1 0.997960090637207\n",
      "1,0 0.9977418184280396\n",
      "0,0 0.0010158593067899346\n",
      "1,1 0.0022931243292987347\n"
     ]
    }
   ],
   "source": [
    "x.set([0,1])\n",
    "print (\"0,1\",output.value())\n",
    "\n",
    "x.set([1,0])\n",
    "print (\"1,0\",output.value())\n",
    "\n",
    "x.set([0,0])\n",
    "print (\"0,0\",output.value())\n",
    "\n",
    "x.set([1,1])\n",
    "print (\"1,1\",output.value())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53565913,  0.69983494],\n",
       "       [ 2.96737266, -3.60019517],\n",
       "       [ 1.21056926,  1.22047853],\n",
       "       [-1.27657628,  0.60889524],\n",
       "       [ 3.27553821, -2.57272363],\n",
       "       [-1.26496565,  0.51268899],\n",
       "       [-0.11418127, -0.21923806],\n",
       "       [ 2.33958912,  2.33326292]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.7094781422615051\n",
      "average loss is: 0.6628954063355923\n",
      "average loss is: 0.5842473302284876\n",
      "average loss is: 0.49666533261537554\n",
      "average loss is: 0.4237849298417568\n",
      "average loss is: 0.3673067914818724\n",
      "average loss is: 0.3234326565957495\n",
      "average loss is: 0.28870834081666547\n",
      "average loss is: 0.26065978465187883\n",
      "average loss is: 0.2375765295624733\n",
      "average loss is: 0.2182665043264966\n",
      "average loss is: 0.20188277295247342\n",
      "average loss is: 0.18781058445931054\n",
      "average loss is: 0.1755943974001067\n",
      "average loss is: 0.16489014871201169\n",
      "average loss is: 0.15543344435398468\n",
      "average loss is: 0.14701792572597588\n",
      "average loss is: 0.13948026080098416\n",
      "average loss is: 0.13268953273636533\n",
      "average loss is: 0.12653960262238978\n",
      "average loss is: 0.1209435277614033\n",
      "average loss is: 0.11582941535479305\n",
      "average loss is: 0.11113730798900613\n",
      "average loss is: 0.10681680545977239\n",
      "average loss is: 0.10282523674741387\n",
      "average loss is: 0.09912623139277388\n",
      "average loss is: 0.0956885992727001\n",
      "average loss is: 0.09248543946138982\n",
      "average loss is: 0.08949342699705398\n",
      "average loss is: 0.08669223800386923\n",
      "average loss is: 0.08406408138077466\n",
      "average loss is: 0.08159331582319282\n",
      "average loss is: 0.07926613453803866\n",
      "average loss is: 0.07707030368126545\n",
      "average loss is: 0.07499494430855183\n",
      "average loss is: 0.07303034959936566\n",
      "average loss is: 0.07116783082422272\n",
      "average loss is: 0.06939958771035125\n",
      "average loss is: 0.06771859671266821\n",
      "average loss is: 0.06611851729199407\n",
      "average loss is: 0.06459361009438885\n",
      "average loss is: 0.06313866784147519\n",
      "average loss is: 0.061748954581697796\n",
      "average loss is: 0.060420153495876795\n",
      "average loss is: 0.05914832175181558\n",
      "average loss is: 0.05792985045378927\n",
      "average loss is: 0.05676143012423207\n",
      "average loss is: 0.05564002038811547\n",
      "average loss is: 0.054562822801431606\n",
      "average loss is: 0.053527257361938245\n",
      "average loss is: 0.05253094182984319\n",
      "average loss is: 0.05157167244689145\n",
      "average loss is: 0.05064740825997982\n",
      "average loss is: 0.04975625585984542\n",
      "average loss is: 0.04889645650068467\n",
      "average loss is: 0.04806637387618788\n",
      "average loss is: 0.047264484376728276\n",
      "average loss is: 0.046489366684510404\n",
      "average loss is: 0.04573969402038729\n",
      "average loss is: 0.045014226033876184\n",
      "average loss is: 0.044311802059347115\n",
      "average loss is: 0.04363133442477368\n",
      "average loss is: 0.04297180339773481\n",
      "average loss is: 0.042332251431671464\n",
      "average loss is: 0.04171177876131752\n",
      "average loss is: 0.04110953885453372\n",
      "average loss is: 0.040524734889101514\n",
      "average loss is: 0.03995661582248217\n",
      "average loss is: 0.039404473345687925\n",
      "average loss is: 0.038867638728498216\n",
      "average loss is: 0.038345480235777205\n",
      "average loss is: 0.03783740042412925\n",
      "average loss is: 0.0373428341506722\n",
      "average loss is: 0.03686124596219334\n",
      "average loss is: 0.036392128547777736\n",
      "average loss is: 0.03593500082733379\n",
      "average loss is: 0.03548940609144984\n",
      "average loss is: 0.03505491075550731\n",
      "average loss is: 0.03463110269267625\n",
      "average loss is: 0.03421759002948238\n"
     ]
    }
   ],
   "source": [
    "# create a network for the xor problem given input and output\n",
    "def create_xor_network(pW, pV, pb, inputs, expected_answer):\n",
    "    dy.renew_cg() # new computation graph\n",
    "    W = dy.parameter(pW) # add parameters to graph as expressions\n",
    "    V = dy.parameter(pV)\n",
    "    b = dy.parameter(pb)\n",
    "    x = dy.vecInput(len(inputs))\n",
    "    x.set(inputs)\n",
    "    y = dy.scalarInput(expected_answer)\n",
    "    output = dy.logistic(V*(dy.tanh((W*x)+b)))\n",
    "    loss =  dy.binary_log_loss(output, y)\n",
    "    return loss\n",
    "\n",
    "m2 = dy.ParameterCollection()\n",
    "pW = m2.add_parameters((8,2))\n",
    "pV = m2.add_parameters((1,8))\n",
    "pb = m2.add_parameters((8))\n",
    "trainer = dy.SimpleSGDTrainer(m2)\n",
    "\n",
    "seen_instances = 0\n",
    "total_loss = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    loss = create_xor_network(pW, pV, pb, question, answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print (\"average loss is:\",total_loss / seen_instances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\n",
      "[1, 2, 3] 1\n",
      "1.6750271320343018\n",
      "[3, 2, 4] 2\n",
      "3.186349868774414\n",
      "[1, 2, 3] 1\n",
      "1.329147458076477\n",
      "[3, 2, 4] 2\n",
      "2.654533624649048\n",
      "[1, 2, 3] 1\n",
      "1.0684216022491455\n",
      "[3, 2, 4] 2\n",
      "2.1631932258605957\n",
      "[1, 2, 3] 1\n",
      "0.868177056312561\n",
      "[3, 2, 4] 2\n",
      "1.7094173431396484\n",
      "[1, 2, 3] 1\n",
      "0.7108930349349976\n",
      "[3, 2, 4] 2\n",
      "1.304443359375\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "m = dy.ParameterCollection()\n",
    "\n",
    "# add parameters to parameter collection\n",
    "pW = m.add_parameters((10,30))\n",
    "pB = m.add_parameters(10)\n",
    "lookup = m.add_lookup_parameters((500, 10))\n",
    "print (\"added\")\n",
    "\n",
    "# create trainer\n",
    "trainer = dy.SimpleSGDTrainer(m)\n",
    "\n",
    "# Regularization is set via the --dynet-l2 commandline flag.\n",
    "# Learning rate parameters can be passed to the trainer:\n",
    "# alpha = 0.1  # learning rate\n",
    "# trainer = dy.SimpleSGDTrainer(m, e0=alpha)\n",
    "\n",
    "# function for graph creation\n",
    "def create_network_return_loss(inputs, expected_output):\n",
    "    \"\"\"\n",
    "    inputs is a list of numbers\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    W = dy.parameter(pW) # from parameters to expressions\n",
    "    b = dy.parameter(pB)\n",
    "    emb_vectors = [lookup[i] for i in inputs]\n",
    "    net_input = dy.concatenate(emb_vectors)\n",
    "    net_output = dy.softmax( (W*net_input) + b)\n",
    "    loss = -dy.log(dy.pick(net_output, expected_output))\n",
    "    return loss\n",
    "\n",
    "# function for prediction\n",
    "def create_network_return_best(inputs):\n",
    "    \"\"\"\n",
    "    inputs is a list of numbers\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    W = dy.parameter(pW)\n",
    "    b = dy.parameter(pB)\n",
    "    emb_vectors = [lookup[i] for i in inputs]\n",
    "    net_input = dy.concatenate(emb_vectors)\n",
    "    net_output = dy.softmax( (W*net_input) + b)\n",
    "    return np.argmax(net_output.npvalue())\n",
    "\n",
    "\n",
    "# train network\n",
    "for epoch in range(5):\n",
    "    for inp,lbl in ( ([1,2,3],1), ([3,2,4],2) ):\n",
    "        print (inp, lbl)\n",
    "        loss = create_network_return_loss(inp, lbl)\n",
    "        print (loss.value()) # need to run loss.value() for the forward prop\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "\n",
    "print (create_network_return_best([1,2,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'model' has incorrect type (expected _dynet.ParameterCollection, got int)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-155ffcebdd9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mHIDDEN_DIM1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mHIDDEN_DIM2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# or:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# builder = dy.SimpleRNNBuilder(NUM_LAYERS, INPUT_DIM, HIDDEN_DIM, pc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'model' has incorrect type (expected _dynet.ParameterCollection, got int)"
     ]
    }
   ],
   "source": [
    "dy.renew_cg()\n",
    "pc = dy.ParameterCollection()\n",
    "NUM_LAYERS=3\n",
    "INPUT_DIM=50\n",
    "HIDDEN_DIM1=10\n",
    "HIDDEN_DIM2=20\n",
    "builder = dy.LSTMBuilder(NUM_LAYERS, INPUT_DIM, HIDDEN_DIM1, pc)\n",
    "# or:\n",
    "# builder = dy.SimpleRNNBuilder(NUM_LAYERS, INPUT_DIM, HIDDEN_DIM, pc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = builder.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = dy.vecInput(INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=s0.add_input(x1)\n",
    "y1 = s1.output()\n",
    "# here, we add x1 to the RNN, and the output we get from the top is y (a HIDEN_DIM-dim vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.npvalue().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2=s1.add_input(x1) # we can add another input\n",
    "y2=s2.output()\n",
    "y2.npvalue().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(expression 63/8016, expression 78/8016, expression 93/8016)\n"
     ]
    }
   ],
   "source": [
    "print( s2.h())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all layers: (expression 22/8016, expression 35/8016, expression 48/8016)\n"
     ]
    }
   ],
   "source": [
    "# create a simple rnn builder\n",
    "rnnbuilder=dy.SimpleRNNBuilder(NUM_LAYERS, INPUT_DIM, HIDDEN_DIM, pc)\n",
    "\n",
    "# initialize a new graph, and a new sequence\n",
    "rs0 = rnnbuilder.initial_state()\n",
    "\n",
    "# add inputs\n",
    "rs1 = rs0.add_input(x1)\n",
    "ry1 = rs1.output()\n",
    "print (\"all layers:\", s1.h())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(expression 14/8015, expression 16/8015)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (s1.s())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN h: (expression 36/8015,)\n",
      "RNN s: (expression 36/8015,)\n",
      "LSTM h: (expression 16/8015,)\n",
      "LSTM s: (expression 14/8015, expression 16/8015)\n"
     ]
    }
   ],
   "source": [
    "rnn_h  = rs1.h()\n",
    "rnn_s  = rs1.s()\n",
    "print (\"RNN h:\", rnn_h)\n",
    "print (\"RNN s:\", rnn_s)\n",
    "\n",
    "\n",
    "lstm_h = s1.h()\n",
    "lstm_s = s1.s()\n",
    "print (\"LSTM h:\", lstm_h)\n",
    "print (\"LSTM s:\", lstm_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
